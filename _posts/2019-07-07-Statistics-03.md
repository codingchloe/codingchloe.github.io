---
layout: post
title: "기초통계학(3)- 분류 모형별 용어/지표 정리"
categories: [Statistics]
---
<hr>

## 목적: 각 클래스에 속할 예측 확률 구하기
1. 어떤 레코드가 속할 거라고 생각되는 관심 클래스에 대한 컷오프 확률을 정한다.
2. 레코드가 관심 클래스에 속할 확률을 (모든 모델과 함께) 추정한다.
3. 그 확률이 컷오프 확률 이상이면 관심 클래스에 이 레코드를 할당한다.

컷오프가 높을수록 1로 예측되는 레코드가 적어질 것이다. 컷오프가 낮을수록 더 많은 레코드가 1로 예측된다.

## 나이브 베이즈
주어진 결과에 대해 예측변수 값을 관찰할 확률을 사용하여, 예측변수가 주어졌을 때, 결과 Y = i를 관찰할 확률을 추정한다.
<br>
* 조건부확률(conditional probability): 어떤 사건(Y=i)이 주어졌을 때, 해당 사건(X=i)을 관찰할 확률
* 사후확률(posterior probability): 예측 정보를 통합한 후 결과의 확률
<br>

1. 이진 응답 Y = i(i=0 또는 i=1)에 대해, 각 예측변수에 대한 조건부확률 P(X_j|Y=i)를 구한다.
이것은 Y=i가 주어질 때, 예측변수의 값이 나올 확률이다. 이 확률은 train set에서 Y = i인 레코드들 중 X_j 값의 비율로 구할 수 있다.
2. 각 확률값을 곱한 다음, Y=i에 속한 레코드들의 비율을 곱한다.
3. 모든 클래스에 대해 1~2단계를 반복한다.
4. 2단계에서 모든 클래스에 대해 구한 확률 값을 모두 더한 값으로 클래스 i의 확률을 나누면 결과 i의 확률을 구할 수 있다.
5. 이 예측변수에 대해 가장 높은 확률을 갖는 클래스를 해당 레코드에 할당한다.

#### 정의에 따라, 베이지언 분류기는 예측변수들이 범주형인 경우에 적합하다. 수치형 변수에 나이브 베이즈 방법론을 적용하기 위해서는<br>
* 쪼개어(binning) 범주형으로 변환한 뒤, 알고리즘을 적용한다.
* 조건부확률, P(X_j|Y=i)를 추정하기 위해 정규분포 같은 확률 모형을 사용한다.


## 판별분석(Discriminant Analysis)
초창기의 통계 분류 방법. 가장 일반적으로 사용되는 것은 선형판별분석(Linear discriminant analysis, LDA)이다. 트리 모델이나 로지스틱과 같은 더 정교한 기법이 출현한 이후로는 그렇게 많이 사용하지 않는다. 하지만 여전히 일부 응용 분야에서는 LDA를 사용하고 있으며, 주성분분석과 같이 아직도 많이 사용되는 다른 방법들과도 연결된다. 또한 판별분석은 예측변수들의 중요성을 측정하거나 효과적으로 특징을 선택하는 방법으로도 사용한다.
* 공분산(covariance) <br>
하나의 변수가 다른 변수와 함께 변화하는 정도(유사한 크기와 방향)를 측정하는 지표<br>
* 판별함수(discriminant function) <br>
예측변수에 적용했을 때, 클래스 구분을 최대화하는 함수<br>
* 판별 가중치(discriminant weight) <br>
판별함수를 적용하여 얻은 점수를 말하며, 어떤 클래스에 속할 확률을 추정하는 데 사용된다.<br>
* 공분산행렬 <br>
식은 위키를 참고하도록 하자. 상관계수처럼 양수는 양의 관계를, 음수는 음의 관계를 나타낸다. 하지만 공분산은 변수 x와 z에서 사 용하는 척도와 동일한 척도에서 정의된다. 공분산행렬을 사용하여 한 클래스와 다른 클래스에 속한 데이터를 구분하는 선형판별함수(linear discriminant function)를 계산할 수 있다.<br>


## 로지스틱 회귀(logistic regression)
y가 이진형 변수. 로지스틱 회귀의 핵심 구성요소는 로지스틱 반응 함수와 로짓이다. 여기서 우리는 확률을 선형 모델링에 적합한 더 확장된 단위로 매핑한다. <br>
* 로짓(logit)/로그오즈: (0~1이 아니라) 무한대 범위에서 어떤 클래스에 속할 확률을 결정하는 함
* 오즈(odds): 실패(0)에 대한 성공(1)의 비율
* 로그 오즈(log odds): 변환 모델(선형)의 응답 변수. 이 값을 통해 확률을 구한다
<br>
1. 결과 변수를 0이나 1이 아닌 라벨이 '1'이 될 확률 p로 생각해보자.
2. 당연히 p를 선형함수로 모델링할 생각을 당신은 할 것이다 .
3. 그러나 이 모델을 피팅한다 해도, 당연히 선형모델이다 보니 p가 0과 1사이로 딱 떨어지지 않을 수 있다. 더 이상 확률이라고 할 수 없게 된다.
4. 대신 예측변수에 역로짓 함수라는 것을 사용하여 p를 모델링한다. 이 변환을 통해 우리는 p가 항상 0에서 1사이에 오도록 할 수 있다.
5. 분모의 지수 부분을 구하려면 확률 대신 오즈비를 이용한다. 오즈비는 사건이 발생할 확률을 사건이 발생하지 않을 확률로 나눈 비율이다. <br>
  Odds(Y=1) = p/(1-p)<br>
  또한 역 오즈비 함수를 통해 확률값을 구할 수도 있다. <br>
  p = odds/(1+odds)<br>
6. 오즈 수식을 로지스틱 함수에 적용하면 다음과 같은 수식을 얻을 수 있다.
  odds(Y=1) = e^(b_0 + b_1*x_1 + b_2*x_2 + ... + b_q*x_q)
7. 양변에 로그 함수를 취하면 우리는 예측변수에 대한 선형함수를 얻을 수 있다.
  log(odds(Y=1)) = b_0 + b_1*x_1 + b_2*x_2 + ... + b_q*x_q


**log odds 또는 logit 함수는 0과 1사이의 확률 p를 -inf에서 +inf까지의 값으로 매핑해준다.**
**어떤 확률을 예측할 수 있는 선형 모형을 구했다.**
**이제 cutoff 기준을 이용해 그 값보다 큰 확률값이 나오면 1로 분률하는 식의 과정을 통해 클래스 라벨을 구할 수 있다.**

* 오즈비(Odds ratio)<br>
logistic regression은 이 오즈비 때문에 다른 분류 방법들에 비해 상대적으로 해석하기가 쉽다. <br>
odds ratio = odds(Y=1|X=1)/odds(Y=1|X=0) <br>

**왜 확률 대신 오즈비를 사용하는 것일까?**
<br>
logistic regression에서 계수 b_j는 X_j에 대한 오즈비의 로그 값이기 때문에 오즈비를 사용한다. <br>
예를 들어 b_2가 1.2라 한다면, 변수 x_2는 exp(1.2), 약 3만큼 성공하는거 대비 성공하지 않을 오즈비가 감소한다는 것을 의미한다.


## 선형 회귀 vs. 로지스틱 회귀 차이점
* **모델을 피팅하는 방식(최소제곱을 사용할 수 없다)**<br>
선형회귀에서는 모델 피팅을 위해 최소제곱을 사용한다. RMSE와 R-squared를 사용하여 피팅의 성능을 평가한다. <br>
로지스틱 회귀분석에서는 닫힌 형태의 해가 없으므로 최대우도추정(Maximum Likelihood Estimation, MLE)을 사용하여 모델을 피팅해야 한다. <u>최대우도추정이란</u>, 우리가 보고 있는 데이터를 생성했을 가능성이 가장 큰 모델을 찾는 프로세스를 말한다. 로지스틱의 응답변수는 응답이 1인 로그 오즈비의 추정치이다. MLE는 예상 로그 오즈비가 관찰된 결과를 가장 잘 설명하는 모델을 찾는다.
<br>

* **모델에서 잔차의 특징과 분석**<br>
로지스틱에서의 편잔차는, 추정결과로 얻은 회귀선을 중심으로 점들이 뭉쳐 있는 구름같은 모양이 두 군데 있다. 로지스틱 회귀에서 편잔차는 회귀에서보다 덜 중요하긴 하지만, 비선형성을 검증하고 영향력이 큰 레코드들을 확인하는 데 여전히 유용하다.


## 모델 평가 지표
* 정확도(accuracy): 정확히 분류된 비율
* 혼동행렬(confusion matrix): 분류에서 예측된 결과와 실제 결과에 대한 레코드의개수를 표시한 2 by 2 테이블
* 민감도(sensitivity, 혹은 재현율(recall)): 정확히 분류된 1의 비율
* 특이도(specificity): 정확히 분류된 0의 비율
* 정밀도(precision): 정확히 1이라고 예측된 1의 비율
* ROC Curve: 민감도와 특이성을 표시한 그림
* 리프트(lift): 모델이 다른 확률 컷오프에 대해 1을 얼마나 더 효과적으로 구분하는지 나타내는 측정 지표
